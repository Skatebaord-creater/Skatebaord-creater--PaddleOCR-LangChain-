import json

from langchain.memory import ConversationBufferMemory
from langchain_community.chat_models import ChatOpenAI
from langchain_community.llms import Ollama
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_core.messages import HumanMessage
import os
from openai import OpenAI

# def generate_next_response(chat_history_path='MorganLeGouar.json'):
#     # è¯»å–èŠå¤©è®°å½•
#     # æ‹¼æ¥å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
#     full_path = os.path.join('assets', chat_history_path)
#     # è¯»å–èŠå¤©è®°å½•
#     chat_context = load_chat_history(full_path)
#     print(chat_context)
#     # è‡ªå®šä¹‰ Prompt æ¨¡æ¿
#     template = """
#     ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½èŠå¤©åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯ç”¨æˆ·ä¸ä¸€ä¸ªå¥³å­©çš„èŠå¤©è®°å½•ï¼š
#
#     {chat_history}
#
#     è¯·åŸºäºè¿™æ®µèŠå¤©ï¼Œç”Ÿæˆä¸€å¥åˆç†çš„ä¸‹ä¸€æ¡å›å¤ï¼š
#     """
#
#     prompt = PromptTemplate(
#         input_variables=["chat_history"],
#         template=template,
#     )
#
#     llm = Ollama(model='mistral')
#
#     chain = LLMChain(llm=llm, prompt=prompt)
#
#     response = chain.run({"chat_history": chat_context})
#
#     print("ğŸ§  ç”Ÿæˆå›å¤ï¼š", response)

# è¯·ç¡®ä¿æ‚¨å·²å°† API Key å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡ ARK_API_KEY ä¸­
# åˆå§‹åŒ–Arkå®¢æˆ·ç«¯ï¼Œä»ç¯å¢ƒå˜é‡ä¸­è¯»å–æ‚¨çš„API Key
# client = OpenAI(
#     # æ­¤ä¸ºé»˜è®¤è·¯å¾„ï¼Œæ‚¨å¯æ ¹æ®ä¸šåŠ¡æ‰€åœ¨åœ°åŸŸè¿›è¡Œé…ç½®
#     base_url="https://ark.cn-beijing.volces.com/api/v3",
#     # ä»ç¯å¢ƒå˜é‡ä¸­è·å–æ‚¨çš„ API Keyã€‚æ­¤ä¸ºé»˜è®¤æ–¹å¼ï¼Œæ‚¨å¯æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹
#     api_key="7836e00a-5fa3-472a-b19b-2b64591c07ea",
# )

# def generate_next_response1(chat_history_path):
#         # full_path = os.path.join('../assets',  chat_history_path)
#          # è¯»å–èŠå¤©è®°å½•
#         chat_context = load_chat_history(chat_history_path)
#         response = client.chat.completions.create(
#             # æŒ‡å®šæ‚¨åˆ›å»ºçš„æ–¹èˆŸæ¨ç†æ¥å…¥ç‚¹ IDï¼Œæ­¤å¤„å·²å¸®æ‚¨ä¿®æ”¹ä¸ºæ‚¨çš„æ¨ç†æ¥å…¥ç‚¹ ID
#             model="doubao-1-5-vision-pro-32k-250115",
#             messages=[
#                 {
#                     "role": "user",
#                     "content": [
#                         {
#                             "type": "text",
#                             "text": f"ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½èŠå¤©åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯å®Œæ•´çš„èŠå¤©è®°å½•ï¼š\n{chat_context}\nè¯·åŸºäºè¿™æ®µå®Œæ•´çš„èŠå¤©ï¼Œç”Ÿæˆä¸€å¥åˆç†çš„ä¸‹ä¸€æ¡å›å¤ï¼š"
#                         }
#                     ]
#                 }
#             ],
#
#         )
#
#         print(f"ğŸ¤–å»ºè®®ä½ å›ç­” ï¼š {response.choices[0].message.content}")

# åˆå§‹åŒ–Arkå®¢æˆ·ç«¯ï¼Œä»ç¯å¢ƒå˜é‡ä¸­è¯»å–æ‚¨çš„API Key
client = ChatOpenAI(
    # æ­¤ä¸ºé»˜è®¤è·¯å¾„ï¼Œæ‚¨å¯æ ¹æ®ä¸šåŠ¡æ‰€åœ¨åœ°åŸŸè¿›è¡Œé…ç½®
    openai_api_base="https://ark.cn-beijing.volces.com/api/v3",
    # ä»ç¯å¢ƒå˜é‡ä¸­è·å–æ‚¨çš„ API Keyã€‚æ­¤ä¸ºé»˜è®¤æ–¹å¼ï¼Œæ‚¨å¯æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹
    openai_api_key="7836e00a-5fa3-472a-b19b-2b64591c07ea",
    model="doubao-1-5-vision-pro-32k-250115"
)

# è‡ªå®šä¹‰ç»“æ„åŒ–æç¤ºæ¨¡æ¿
myPromptTemplate = PromptTemplate(
    input_variables=["chat_history"],
    template="""
ä½ æ˜¯ä¸€ä¸ªæ“…é•¿ç¤¾äº¤å¿ƒç†å­¦çš„æ‹çˆ±èŠå¤©é¡¾é—®ï¼Œè§’è‰²æ˜¯â€œæ‡‚å¥¹åˆæ‡‚æ’©çš„æœ‹å‹â€ â€”â€” æ—¢èƒ½è¯»æ‡‚å¥¹çš„æƒ…ç»ªï¼Œåˆèƒ½åˆ¶é€ æš§æ˜§å¿ƒåŠ¨çš„æ°›å›´ã€‚

ğŸ¯ ä½ çš„ç›®æ ‡æ˜¯ï¼š
- è®©å¥¹æ„Ÿå—åˆ°è¢«ç†è§£ã€è¢«æ¥çº³ï¼Œæƒ…ç»ªä¸Šå¯¹ä½ é€æ¸ä¾èµ–ï¼›
- è½»æ¾æ’©åŠ¨å¥¹çš„å¿ƒå¼¦ï¼Œå»ºç«‹æš§æ˜§å¼ åŠ›ï¼›
- ä¸æ²¹è…»ã€ä¸è®¨å¥½ï¼Œè®©å¥¹é€æ¸â€œä¸Šå¤´â€ï¼Œå–œæ¬¢ä¸Šä½ ã€‚

ğŸ§  è¯·åˆ†æä»¥ä¸‹èŠå¤©è®°å½•ï¼Œå¹¶ç”Ÿæˆä¸€æ¡å›å¤ï¼Œä½¿å¥¹**æ„Ÿè§‰ä½ ç‰¹åˆ«æ‡‚å¥¹ï¼ŒåŒæ—¶äº§ç”Ÿæƒ…ç»ªä¾èµ–ä¸å¥½å¥‡æ„Ÿï¼Œæ„¿æ„ç»§ç»­äº²å¯†å¯¹è¯ã€‚**

ğŸ’¬ èŠå¤©è®°å½• Chat History:
\"\"\"
{chat_history}
\"\"\"

ğŸ“Œ è¯·ä½¿ç”¨ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼š
1. âœ¨ å›å¤èƒŒåçš„æƒ…ç»ªç­–ç•¥ï¼ˆå¦‚ï¼šå…±æƒ… + èµç¾ / åå·®å¹½é»˜ / æ’©è€Œä¸ä¿— / å®‰å…¨æ„Ÿ + å°æŒ‘é€—ï¼‰
2. â¤ï¸ æ¨èå›å¤å†…å®¹ï¼ˆä¸€å¥è¯æˆ–ä¸¤å¥è¯ï¼Œè½»æ’©+æ‡‚å¥¹+åˆ¶é€ å¼ åŠ›ï¼‰
"""
)


def load_chat_history(chat_history_path):
    """ä»JSONæ–‡ä»¶åŠ è½½èŠå¤©å†å²å¹¶è½¬æ¢ä¸ºLangChainæ¶ˆæ¯æ ¼å¼"""
    with open(chat_history_path, 'r', encoding='utf-8') as f:
        chat_data = json.load(f)

    # åˆå§‹åŒ–Memoryå¯¹è±¡
    memory = ConversationBufferMemory(memory_key="chat_history")

    # å°†æ¯æ¡æ¶ˆæ¯æ·»åŠ åˆ°Memoryä¸­
    for msg in chat_data:
        if msg["role"] == "å¯¹æ–¹":
            memory.chat_memory.add_user_message(msg["text"])
        elif msg["role"] == "æˆ‘":
            memory.chat_memory.add_ai_message(msg["text"])

    return memory
def generate_next_response1(chat_history_path):
    # è¯»å–èŠå¤©è®°å½•
    chat_context = load_chat_history(chat_history_path)

    # æ„é€  Chain
    chain = LLMChain(llm=client, prompt=myPromptTemplate)

    # è¿è¡Œ Chainï¼Œç”Ÿæˆå›å¤
    response = chain.run(chat_history=chat_context)

    print("ğŸ¤–å»ºè®®ä½ å›ç­”ï¼š\n", response)

